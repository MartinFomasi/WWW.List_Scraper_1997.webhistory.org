---
title: "Fomasi, M. (2024). WWWHTML_Scraper (Versione 1.0) [R; Windows (64x)]"
author: "Martin Fomasi"
date: "2024-09-29"
Legal_info_WWWHTML: "Copyright Â© 1996-2003 The World Wide Web History Project and Arcady Press"
For_more_legal_info: "http://1997.webhistory.org/legal.html"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Install packages and open libraries}
if (!require("rvest")) install.packages("rvest")
library(rvest)

if (!require("httr2")) install.packages("httr2")
library(httr2)

if (!require("dplyr")) install.packages("dplyr")
library(dplyr)

if (!require("stringr")) install.packages("stringr")
library(stringr)

if (!require("tidyr")) install.packages("tidyr")
library(tidyr)

if (!require("fs")) install.packages("fs")
library(fs)

if (!require("openxlsx")) install.packages("openxlsx")
library(openxlsx)

if (!require("lubridate")) install.packages("lubridate")
library("lubridate")

if (!require("ggplot2")) install.packages("ggplot2")
library("ggplot2")
```

```{r Scraping Directories (METADATA)}
                                                    
URL<- c("http://1997.webhistory.org/www.lists/www-html.1994q2/subject.html#start", 
        "http://1997.webhistory.org/www.lists/www-html.1994q3/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-html.1994q4/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-html.1995q1/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-html.1995q2/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-html.1995q3/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-html.1995q4/subject.html#start")

```


```{r Loop to read the HTML and convert in character} 

list_html<-list() #Create an empty list

for (i in 1:7) { #Start the loop
  
  HTML <- read_html(URL[i]) #Read html
  STRING <- as.character(HTML) #Convert in character
  
  list_html[i]<-STRING #Save in the list
  
} #End of the loop

Copy<-list_html #Copy of the list

HTML_df<-do.call(rbind, Copy) #Convert the copy in a dataframe
```

```{r Create list of Titles}

list_Raw<-list() #Create an empty list to store raw TITLES

for(i in 1:7){ #Start the loop
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("<b>(.*?)</b>", HTML_df[i])))
  
  testo_b<-testo_b[-(1:5),] #Remove Headers
  
  testo_b <- head(testo_b, -4) # Remove Footers
  
  list_Raw[[i]]<-testo_b #Save the results
  
}

Titles<-list_Raw #Save Titles results
```

```{r Create a list of RAW titles, authors, dates}

list_Author<-list() #Create an empty list to store raw TITLES and AUTHORS

for(i in 1:7){ #Start the loop
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("<a href(.*?)<b>", HTML_df[i]))) #Select the content between each title
  
  testo_b<-testo_b[-(1:2),] #Remove the headers
  
  testo_b <- head(testo_b, -1) # Remove the footers
  
  list_Author[[i]]<-testo_b #Save the results
  
}

Raw_Authors<-list_Author #Copy the list
```

```{r Extract AUTHORS of each thread (title of the message)}

list_Authors_clean<-list()#new empty list

for (i in 1:7) {
  
  temporary<-str_extract_all(Raw_Authors[[i]], 'html\">(.*?)</a>') #extract authors
  
  list_Authors_clean[[i]]<-temporary #Save the results
  
}

Authors<-list_Authors_clean #Copy the list
```

```{r Extract DATES of the messages}

Date_list<-list()

for (i in 1:7) {
  
  temporary<-str_extract_all(Raw_Authors[[i]], '<i>(.*?)</i>') #extract authors
  
  Date_list[[i]]<-temporary #Save the result
  
  Date<-Date_list #Copy the list
}
```

```{r, cache=TRUE}

Authors[[2]] <- c(Authors[[2]], Authors[[3]][1])

Authors[[3]] <- Authors[[3]][-1]

Titles[[2]] <- append(Titles[[2]], "Re: Error Condition Re:")

Date[[2]]<-c(Date[[2]], Date[[3]][1])#Move First Date of 1994q2 as last Date of 1994q1

Date[[3]] <- Date[[3]][-1]


Authors[[5]] <- c(Authors[[5]], Authors[[6]][1])

Authors[[6]] <- Authors[[6]][-1]

Titles[[5]] <- append(Titles[[5]], "Re: Error Condition Re:")

Date[[5]]<-c(Date[[5]], Date[[6]][1])#Move First Date of 1994q2 as last Date of 1994q1

Date[[6]] <- Date[[6]][-1]

```

```{r Create Directories Dataframe and List}

Storing_list<-list()

for (i in 1:7) {   #Start main loop
  
  Merging_list<-list() #Create an empty list to store the results of the the sub loop
  
  for ( x in 1:length(Authors[[i]])) { #Start sub loop
    
    Test<- data.frame(Authors[[i]][[x]]) #Import authors
    
    colnames(Test)[1] <- "Authors" #Rename first column in Authors
    
    current<-Titles[[i]] #Select current Titles
    
    Test[2]<- current[x] #Import Titles
    
    colnames(Test)[2]<- "Title"#Rename second column in Title 
    
    Merging_list[[x]]<-Test #Save iteration 
    
    current<-Date[[i]] #Select date
    
    Test[3]<-current[x] #Import date
    
    colnames(Test)[3]<-"Date" #Rename column
    
    Merging_list[[x]]<-Test #Save the result of the sub loop
  }
  
  Storing_list[[i]]<-Merging_list #Save sub loop into the main list and ressta
  
}

Df<-bind_rows(Storing_list)

```

```{r Clean the dataframe. ATTENTION !!! The cleaning produce a WARNING MESSAGES. It could be ignored.}

Df_cleaning<-Df%>%
  mutate(Authors=gsub('html">', "", Authors))%>%  #Remove html"> from Authors
  mutate(Authors=gsub("</a>", "", Authors))%>%    #Remove <a/> from Authors
  mutate(Title=gsub("<b>","",Title))%>%           #Remove <b> from Title
  mutate(Title=gsub("</b>","",Title))%>%          #Remove </b> from Title
  mutate(Date=gsub("<i>","",Date))%>%             #Remove <i> from Date
  mutate(Date=gsub("</i>","",Date))%>%            #Remove <i/> from Date
  mutate(Date=gsub("  "," ", Date))%>%            #Replace a double space with a single space
  mutate(Complex=Date)%>%                         #Copy Date column
  separate(Complex, into = c("Complex", "Var1", "Var2", "Var3","Var4","Var5", "Var6"), sep = " ", fill = "right")%>% #Separate Complex dates in columns on the right
  select(-"Complex")%>%                           #Remove Complex 
  unite("Combined", Var1, Var2, Var3, sep="-")%>%
  unite(Var5, Var5,Var6, sep=" ")%>%#Merge columns with date elements
  mutate(Combined=gsub("-19","-", Combined))%>%   #Remove 19 before the year
  mutate(Combined = ifelse(grepl("^[1-9]-", Combined), paste0("0", Combined), Combined))%>% #Add a 0 if the day is < 10
  rename(Time=Var4, Time_zone=Var5)%>%# Usa mutate_all per rimuovere "NA" attaccato alla fine di qualsiasi stringa
  mutate(Time_zone=gsub("_NA","", Time_zone))%>%
  mutate(Time_zone=gsub("NA","", Time_zone))%>%
  mutate(Time = ifelse(grepl("^\\d{2}:\\d{2}$", Time), paste0(Time, ":00"), Time))%>%
  mutate(Time = ifelse(grepl("^\\d{1}:\\d{2}:\\d{2}$", Time), paste0("0", Time), Time))
  #mutate_all(~ gsub("_NA$", "", .))%>%
  #mutate_all(~ gsub("NA", "", .))%>%
  #mutate_all(~ gsub("-NA$", "", .))

Problematic_date<-Df_cleaning[nchar(Df_cleaning$Combined) != 9, ]%>%
  select(-Time, -Time_zone)%>%
  separate(Date, into = c("Combined", "Var1", "Var2", "Var3", "Var4", "Var5"), sep = " ", fill = "right")%>% 
  unite("Combined", Combined, Var1, Var2, sep="-")%>%
  unite(Var4, Var4, Var5, sep=" ")%>%
  rename(Time=Var3, Time_zone=Var4)%>%
  mutate(Combined = ifelse(grepl("^[1-9]-", Combined), paste0("0", Combined), Combined))%>%
  mutate(Combined=gsub("-19","-", Combined))%>%# Usa mutate_all per rimuovere "NA" attaccato alla fine di qualsiasi stringa
  mutate(Combined=gsub("--","", Combined))# Usa mutate_all per rimuovere "NA" attaccato alla fine di qualsiasi stringa

index_problematic <- which(nchar(Df_cleaning$Combined) != 9) #Create the index of problematic dates

Df_cleaning$Combined[index_problematic]<-Problematic_date$Combined
Df_cleaning$Time[index_problematic]<-Problematic_date$Time
Df_cleaning$Time_zone[index_problematic]<-Problematic_date$Time_zone

Problematic_date<-Df_cleaning[nchar(Df_cleaning$Combined) != 9, ]

trimws(Df_cleaning$Time)

index_problematic <- which(nchar(Df_cleaning$Time) != 8)

Problematic_time <- Df_cleaning[index_problematic, ]%>%
  mutate(Time = ifelse(grepl("^\\d{2}:\\d{2}$", Time), paste0(Time, ":00"), Time))%>%
  mutate(Time = str_replace(Time, "([+-])", " \\1 ")) %>%  # Aggiungi spazi attorno ai separatori
  separate(Time, into = c("Var1", "Var2"), sep = " ", fill = "right", extra = "merge")%>%
  select(-Time_zone)%>%
  rename(Time=Var1, Time_zone=Var2)%>%
  mutate(Time_zone=gsub(" ","", Time_zone))


Df_cleaning$Time[index_problematic]<-Problematic_time$Time
Df_cleaning$Time_zone[index_problematic]<-Problematic_time$Time_zone

Problematic_time<-Df_cleaning[nchar(Df_cleaning$Time) !=8, ]

Df_cleaning<-Df_cleaning%>%
  mutate_all(~ gsub("_NA$", "", .))%>%
  mutate_all(~ gsub("NA", "", .))%>%
  mutate_all(~ gsub("-NA$", "", .))

Directories<-Df_cleaning #Copy the Dataframe 9982

```

```{r Create the URL of each message}

base<- c("http://1997.webhistory.org/www.lists/www-html.1994q2/",
         "http://1997.webhistory.org/www.lists/www-html.1994q3/",
         "http://1997.webhistory.org/www.lists/www-html.1994q4/",
         "http://1997.webhistory.org/www.lists/www-html.1995q1/",
         "http://1997.webhistory.org/www.lists/www-html.1995q2/",
         "http://1997.webhistory.org/www.lists/www-html.1995q3/",
         "http://1997.webhistory.org/www.lists/www-html.1995q4/")
         
list_URL<-list()

for(i in 1:7){ #Start the loop to extract URL number
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("a href=(.*?)>", HTML_df[i])))
  
  testo_b<-testo_b[-(1:4),] #Remove Headers
  
  testo_b <- head(testo_b, -5) # Remove Footers
  
  testo_b<-gsub("a href=","",testo_b) #Set URL start
  
  testo_b<-gsub(">","",testo_b) #Set URL end
  
  testo_b<-as.data.frame(testo_b) #Convert into a dataframe 
  
  list_URL[[i]]<-testo_b #Save the dataframe
}


URL_list<-list() #Create an empty list

for (i in 1:7) { #Start the loop to create the URLs
  
  temp<-data.frame(list_URL[i])%>% #Create a temporary variable
    mutate(Url=paste0(base[i], testo_b))%>% #Merge URL base with URL number
    mutate(Url=gsub('"',"",Url)) #Remove " symbol 
  
  URL_list[[i]]<-temp #Save the results
}


URL_df<-do.call(rbind, URL_list) #Convert in a dataframe
```

```{r Merge URL in the dataframe and rename columns}

Directories<-Directories%>% 
  mutate(Url=URL_df$Url)

colnames(Directories)[3] <-"Summary"

colnames(Directories)[4]<-"Date"

```

```{r Scraping messages (DATA)}
Metadata <- Directories #Import Excel Metadata

URL<- Metadata$Url # Create a vector of links excluding the problematic link

```

```{r Loop to scrape}

message("Beginning of the loop: ", Sys.time())

List_Text <- list() # Creation of an empty list for storing the forthcoming results

for (i in 1:2819) {  # Select quantity of links
  
  Test <- read_html(URL[i]) # Read the HTML 
  
  Body<-as.character(Test) # Convert the HTML in Character
  
  Cleaned <- sub(".*<!-- body=\"start\" -->(.*)<!-- body=\"end\" -->.*", "\\1", Body) # Select the content of the messages
  
  Cleaned <- str_replace_all(Cleaned, "<[^>]+>", "") # Remove HTML tags
  
  Cleaned <- gsub("\n", " ", Cleaned) # Remove \n tag
  
  List_Text[[i]] <- Cleaned # Add the message to the list
}

message("End of the loop: ", Sys.time())

List_Loop<-List_Text # Copy the list with the Scraping

```

```{r Complete the dataframe}

Text_df<- data.frame(Scraping = unlist(List_Loop)) #Convert the list in a dataframe

Data <- Text_df # Create a copy of the messages (DATA) 

Scraping_df <- cbind(Metadata, Data) # Merge Data and Metadata.

```

```{r Complete the dataframe}

Scraping_df <- Scraping_df %>%
  mutate(Scraping = gsub("&gt;", ">", Scraping))%>% #Remove exceeding HTML tag in the Scraping
  mutate(File_name = gsub("http://1997.webhistory.org/www.lists/", "", Url))%>% #Create a column with an univocal file name based on URL
  mutate(File_name = gsub("/", ".", File_name))%>% #Remove special characters not allowed by Windows for saving
  select(File_name, Authors, Title, Date, Time, Time_zone, Scraping, Url, everything())

Scraping_df$Date<-dmy(Scraping_df$Date)

Problematic_time<-Scraping_df%>%  #Find and solve problematic time
  mutate(Time = str_trim(Time))%>% #Remove space
  filter(!grepl("\\d", Time))


View(Scraping_df)
```
```{r Find empty messages}

emptyLine<-Scraping_df[is.na(Scraping_df$Scraping) |  Scraping_df$Scraping =="", ] #Find empty observation. 20 observation are empty (Qualitatively verified) 
```

```{r Saving as rds and txt}

#Saving as .rds
saveRDS(Scraping_df, file = "WWWHTML_df.rds")

# Excel delle Directories

Directories<-Directories%>%
  mutate(File_name=Scraping_df$File_name)%>%
  select(File_name, everything())

Directories$Date<-dmy(Directories$Date)

write.xlsx(Directories, file = "Directories_HTML.xlsx") #Create an Excel of the Directories

#Saving as txt in a subfolder
output_dir <- "WWWHTML_Text" #Set the subfolder's directory

dir.create(output_dir) # Create the subfolder's directory

for (i in 1:2819) { # Loop for saving file with file name (final part of the URL, without "/") 
  file_name <- file.path(output_dir, paste0(Scraping_df$File_name[i], ".txt"))
  writeLines(Scraping_df$Scraping[i], file_name)
}

message("End of the code: ", Sys.time())
```

```{r Saving as rds and txt}

Timeline<- Scraping_df %>%
  mutate(Date_Ym = format(Date, "%Y-%m"))%>%  # Formatta la data come "Anno-Mese"
  group_by(Date_Ym) %>%                        # Raggruppa per mese e anno
  summarise(Tot_Month = n())   

ggplot(Timeline, aes(x = Date_Ym, y = Tot_Month)) +             # Create a timeline
  geom_bar(stat = "identity", width = 0.65) +                    # Choose bars with adjusted width
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +    # Custom axis
  geom_text(aes(label = Tot_Month), vjust = -0.3, size = 2.5) + # Add values' label
  xlab("Timeline (Y-m)") +                                       # Title x axis
  ylab("Quantity of messages") +                                 # Title y axis
  ggtitle("Message distribution over time") +                    # Title graph
  scale_x_discrete(limits = Timeline$Date_Ym) # Set limits for x axis

```

```{r}
Time_zone<-Scraping_df%>%
count(Time_zone)%>%
  arrange(desc(n))

Scraping_df$ISO_Date <- format(
  as.POSIXct(Scraping_df$Summary, format="%a, %d %b %Y %H:%M:%S %z"),
  "%Y-%m-%dT%H:%M:%S"
)

```

