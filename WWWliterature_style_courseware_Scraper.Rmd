---
title: "Fomasi, M. (2024). WWWliterature_style_courseware (Versione 1.0) [R; Windows (64x)]"
author: "Martin Fomasi"
date: "2024-09-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Install packages and open libraries}
if (!require("rvest")) install.packages("rvest")
library(rvest)

if (!require("httr2")) install.packages("httr2")
library(httr2)

if (!require("dplyr")) install.packages("dplyr")
library(dplyr)

if (!require("stringr")) install.packages("stringr")
library(stringr)

if (!require("tidyr")) install.packages("tidyr")
library(tidyr)

if (!require("fs")) install.packages("fs")
library(fs)

if (!require("openxlsx")) install.packages("openxlsx")
library(openxlsx)

if (!require("lubridate")) install.packages("lubridate")
library("lubridate")
```

```{r Scraping Directories (METADATA)}
                                                    
URL<- c("http://1997.webhistory.org/www.lists/www-literature/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-courseware/subject.html#start",
        "http://1997.webhistory.org/www.lists/www-style.1995/subject.html#start")

```


```{r Loop to read the HTML and convert in character} 

list_html<-list() #Create an empty list

for (i in 1:3) { #Start the loop
  
  HTML <- read_html(URL[i]) #Read html
  STRING <- as.character(HTML) #Convert in character
  
  list_html[i]<-STRING #Save in the list
  
} #End of the loop

Copy<-list_html #Copy of the list

HTML_df<-do.call(rbind, Copy) #Convert the copy in a dataframe
```

```{r Create list of Titles}

list_Raw<-list() #Create an empty list to store raw TITLES

for(i in 1:3){ #Start the loop
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("<b>(.*?)</b>", HTML_df[i])))
  
  testo_b<-testo_b[-(1:5),] #Remove Headers
  
  testo_b <- head(testo_b, -4) # Remove Footers
  
  list_Raw[[i]]<-testo_b #Save the results
  
}

Titles<-list_Raw #Save Titles results
```

```{r Create a list of RAW titles, authors, dates}

list_Author<-list() #Create an empty list to store raw TITLES and AUTHORS

for(i in 1:3){ #Start the loop
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("<a href(.*?)<b>", HTML_df[i]))) #Select the content between each title
  
  testo_b<-testo_b[-(1:2),] #Remove the headers
  
  testo_b <- head(testo_b, -1) # Remove the footers
  
  list_Author[[i]]<-testo_b #Save the results
  
}

Raw_Authors<-list_Author #Copy the list
```

```{r Extract AUTHORS of each thread (title of the message)}

list_Authors_clean<-list()#new empty list

for (i in 1:3) {
  
  temporary<-str_extract_all(Raw_Authors[[i]], 'html\">(.*?)</a>') #extract authors
  
  list_Authors_clean[[i]]<-temporary #Save the results
  
}

Authors<-list_Authors_clean #Copy the list
```

```{r Extract DATES of the messages}

Date_list<-list()

for (i in 1:3) {
  
  temporary<-str_extract_all(Raw_Authors[[i]], '<i>(.*?)</i>') #extract authors
  
  Date_list[[i]]<-temporary #Save the result
  
  Date<-Date_list #Copy the list
}
```

```{r Create Directories Dataframe and List}

Storing_list<-list()

Folder<-c("literature","courseware","style")


for (i in 1:3) {   #Start main loop
  
  Merging_list<-list() #Create an empty list to store the results of the the sub loop
  
  for ( x in 1:length(Authors[[i]])) { #Start sub loop
    
    Test<- data.frame(Authors[[i]][[x]]) #Import authors
    
    colnames(Test)[1] <- "Authors" #Rename first column in Authors
    
    current<-Titles[[i]] #Select current Titles
    
    Test[2]<- current[x] #Import Titles
    
    colnames(Test)[2]<- "Title"#Rename second column in Title 
    
    Merging_list[[x]]<-Test #Save iteration 
    
    current<-Date[[i]] #Select date
    
    Test[3]<-current[x] #Import date
    
    colnames(Test)[3]<-"Date"
    
    Test[4]<-Folder[i]#Rename column
    
    colnames(Test)[4]<-"Folder"
    
    Merging_list[[x]]<-Test #Save the result of the sub loop
  }
  
  Storing_list[[i]]<-Merging_list #Save sub loop into the main list and ressta
  
}

Df<-bind_rows(Storing_list)

```

```{r Clean the dataframe. ATTENTION !!! The cleaning produce a WARNING MESSAGES. It could be ignored.}

Df_cleaning<-Df%>%
  mutate(Authors=gsub('html">', "", Authors))%>%  #Remove html"> from Authors
  mutate(Authors=gsub("</a>", "", Authors))%>%    #Remove <a/> from Authors
  mutate(Title=gsub("<b>","",Title))%>%           #Remove <b> from Title
  mutate(Title=gsub("</b>","",Title))%>%          #Remove </b> from Title
  mutate(Date=gsub("<i>","",Date))%>%             #Remove <i> from Date
  mutate(Date=gsub("</i>","",Date))%>%            #Remove <i/> from Date
  mutate(Date=gsub("  "," ", Date))%>%            #Replace a double space with a single space
  mutate(Complex=Date)%>%                         #Copy Date column
  separate(Complex, into = c("Complex", "Var1", "Var2", "Var3"), sep = " ", fill = "right")%>% #Separate Complex dates in columns on the right
  select(-"Complex")%>%                           #Remove Complex 
  unite("Combined", Var1, Var2, Var3, sep="-")%>% #Merge columns with date elements
  mutate(Combined=gsub("-19","-", Combined))%>%   #Remove 19 before the year
  mutate(Combined = ifelse(grepl("^[1-9]-", Combined), paste0("0", Combined), Combined)) #Add a 0 if the day is < 10. 

Problematic_date<-Df_cleaning[nchar(Df_cleaning$Combined) != 9, ]%>% #Select date that doesn't have 9 characters. 
  separate(Date, into = c("Combined", "Var1", "Var2", "Var3"), sep = " ", fill = "right")%>% #Separate Combined dates in colums on the right
  unite("Combined", Combined, Var1, Var2, sep="-")%>% #Merge columns with date elements
  mutate(Combined=gsub("-19","-", Combined))%>% #Remove 19 from years
  select(-"Var3")%>%   #Remove column 
  mutate(Combined = ifelse(grepl("^[1-9]-", Combined), paste0("0", Combined), Combined)) #Add a 0 if the day is < 10. 

index_problematic <- which(nchar(Df_cleaning$Combined) != 9) #Create the index of problematic dates

Df_cleaning$Combined[index_problematic]<-Problematic_date$Combine #Import corrected problematic dates

Df_cleaning<-Df_cleaning%>%
  mutate(Combined=gsub("-NA-NA","",Combined))

Directories<-Df_cleaning #Copy the Dataframe 9982

```

```{r Create the URL of each message}

base<- c("http://1997.webhistory.org/www.lists/www-literature/",
        "http://1997.webhistory.org/www.lists/www-courseware/",
        "http://1997.webhistory.org/www.lists/www-style.1995/")
         
list_URL<-list()

for(i in 1:3){ #Start the loop to extract URL number
  
  testo_b <- data.frame(regmatches(HTML_df[i], gregexpr("a href=(.*?)>", HTML_df[i])))
  
  testo_b<-testo_b[-(1:4),] #Remove Headers
  
  testo_b <- head(testo_b, -5) # Remove Footers
  
  testo_b<-gsub("a href=","",testo_b) #Set URL start
  
  testo_b<-gsub(">","",testo_b) #Set URL end
  
  testo_b<-as.data.frame(testo_b) #Convert into a dataframe 
  
  list_URL[[i]]<-testo_b #Save the dataframe
}


URL_list<-list() #Create an empty list

for (i in 1:3) { #Start the loop to create the URLs
  
  temp<-data.frame(list_URL[i])%>% #Create a temporary variable
    mutate(Url=paste0(base[i], testo_b))%>% #Merge URL base with URL number
    mutate(Url=gsub('"',"",Url)) #Remove " symbol 
  
  URL_list[[i]]<-temp #Save the results
}


URL_df<-do.call(rbind, URL_list) #Convert in a dataframe
```

```{r Merge URL in the dataframe and rename columns}

Directories<-Directories%>% 
  mutate(Url=URL_df$Url)

colnames(Directories)[3] <-"Summary"

colnames(Directories)[5]<-"Date"

```

```{r Scraping messages (DATA)}
Metadata <- Directories #Import Excel Metadata

URL<- Metadata$Url # Create a vector of links excluding the problematic link

```

```{r Loop to scrape}

message("Beginning of the loop: ", Sys.time())

List_Text <- list() # Creation of an empty list for storing the forthcoming results

for (i in 1:607) {  # Select quantity of links
  
  Test <- read_html(URL[i]) # Read the HTML 
  
  Body<-as.character(Test) # Convert the HTML in Character
  
  Cleaned <- sub(".*<!-- body=\"start\" -->(.*)<!-- body=\"end\" -->.*", "\\1", Body) # Select the content of the messages
  
  Cleaned <- str_replace_all(Cleaned, "<[^>]+>", "") # Remove HTML tags
  
  Cleaned <- gsub("\n", " ", Cleaned) # Remove \n tag
  
  List_Text[[i]] <- Cleaned # Add the message to the list
}

List_Loop<-List_Text # Copy the list with the Scraping

message("End of the lopp: ", Sys.time())

```

```{r Complete the dataframe}

Text_df<- data.frame(Scraping = unlist(List_Loop)) #Convert the list in a dataframe

Data <- Text_df # Create a copy of the messages (DATA) 

Scraping_df <- cbind(Metadata, Data) # Merge Data and Metadata.

```

```{r Complete the dataframe}

Scraping_df <- Scraping_df %>%
  mutate(Scraping = gsub("&gt;", ">", Scraping))%>% #Remove exceeding HTML tag in the Scraping
  mutate(File_name = gsub("http://1997.webhistory.org/www.lists/", "", Url))%>% #Create a column with an univocal file name based on URL
  mutate(File_name = gsub("/", ".", File_name))%>% #Remove special characters not allowed by Windows for saving
  select(File_name, Authors, Title, Date, Scraping, Url, everything())

Scraping_df$Date <-dmy(Scraping_df$Date) #Convert Date in format ISO 8601 YYYY-MM-DD
message("11 failed to parse: 11 messages with missing dates")

View(Scraping_df)
```
```{r Find empty messages}

emptyLine<-Scraping_df[is.na(Scraping_df$Scraping) |  Scraping_df$Scraping =="", ] #Find empty observation. 20 observation are empty (Qualitatively verified) 
```

```{r Saving as rds and txt}

#Saving as .rds

saveRDS(Scraping_df, file = "WWWHTML_df.rds")

# Excel delle Directories


dir<-c("WWWliterature", "WWWWcourseware", "WWWStyle")


for (i in 1:3) { 
  
Folder2<-Folder  
  
current<-Scraping_df%>%
  filter(Folder%in%Folder2[i])

#Saving as txt in a subfolder
output_dir <-dir[i] #Set the subfolder's directory

dir.create(output_dir) # Create the subfolder's directory

for (x in 1:nrow(current) ) { # Loop for saving file with file name (final part of the URL, without "/") 
  file_name <- file.path(output_dir, paste0(current$File_name[x], ".txt"))
  writeLines(current$Scraping[x], file_name)
 }
}
message("End of the code: ", Sys.time())
```





